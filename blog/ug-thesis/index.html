<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Automating Repetetive Work With Computer Vision | Noah Ripstein </title> <meta name="author" content="Noah Ripstein"> <meta name="description" content="My journey towards automating 250 hours of video labelling for my undergrad thesis lab &lt;br&gt;&lt;br&gt;(And don't worry, there's a 1 minute TLDR overview)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nripstein.github.io/blog/ug-thesis/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Noah</span> Ripstein </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Automating Repetetive Work With Computer Vision</h1> <p class="post-meta"> Created in December 20, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="d-flex justify-content-center mt-3"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" controls=""> <source src="/assets/video/segmentation.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <h1 id="1-minute-tldr">1 minute TLDR</h1> <p>In my undergraduate honors thesis in computational neuroscience, I identified an opportunity to apply computer vision to automate 250+ hours of manual video analysis, streamlining a labor-intensive process. My lab focused on the “Bayesian brain hypothesis” and how humans subconsciously use Bayesian inference to form internal representations of probability distrubutions during sensory perception and learning. Participants in our study learned to classify novel objects based on their sense of touch through hundreds of repeated trials. I aimed to automatically segment video of participants performing the experiment into “during-trial” and “between-trial” regions to analyze the duration of hand-object contact.</p> <h4>Key Achievements:</h4> <ol> <li> <b>First Attempt: Frame Classification and Time-Series Segmentation</b> <ul> <li>Used an image classification model to determine probability of "hand-object contact" frame by frame.</li> <li>Combined these frame-wise predictions with the PELT algorithm for time-series segmentation. Initial results were promising, but the image classifier faced challenges generalizing to unseen participants.</li> </ul> </li> <li> <b>Second Attempt: Object Detection and Post-Processing</b> <ul> <li>Leveraged an object detection model trained on a public dataset, avoiding the need for hand-labelling training data.</li> <li>Developed domain-specific post-processing steps reduce false positives.</li> <li>Achieved 87% frame-wise classification accuracy and very strong qualitative results.</li> <li>Encountered "over-segmentation errors," causing poor performance on temporally-sensitive evaluation metrics.</li> </ul> </li> <li> <b>Promising future directions:</b> <ul> <li>Proposed a method to use existing models, which are trained on public datasets, to generate high quality training for specialized task-specific models based on open source models.</li> <li>I strongly suspect that carefully extracting data in the way I suggest for use as training data in the semi-supervised model I suggest would succesfully convert the strong frame-wise performance to strong whole-sequence performance.</li> </ul> </li> </ol> <h4>Results:</h4> <p>The prototype exhibited strong qualitative results, showing clear potential to automate video segmentation. Over-segmentation errors and trial start prediction inaccuracies highlight areas for refinement. Future directions include integrating temporally-aware open source models through the pipeline I propose which leverages automated data labeling and semi-supervised learning.</p> <p>This project demonstrates the value of cross-disciplinary problem-solving, combining various areas of machine learning and statistics (human-object interaction, temporal action segmentation, time series segmationation), to answer scientifically-relevant questions.</p> <div class="text-center mt-4"> <img src="/assets/img/sv2_tas.png" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Ground truth vs predicted by algorithm. The red and green sections line up well in the top and bottom figures, highlighting the method's strong performance</figcaption> </div> <p><br> <br> <br></p> <h1>Table of Contents</h1> <ul> <li><a href="#1-minute-tldr">1-Minute TLDR</a></li> <li> <a href="#what-was-the-lab-studying">What was the lab studying?</a> <ul> <li><a href="#experiment-overview">Experiment Overview</a></li> <li><a href="#stimuli-and-task">Stimuli and Task</a></li> <li><a href="#motivation-for-automation">Motivation for Automation</a></li> </ul> </li> <li><a href="#my-goal">My Goal</a></li> <li><a href="#preliminary-work-automatic-video-zooming">Preliminary Work: Automatic Video Zooming</a></li> <li> <a href="#solution-attempt-1-unsuccessful">Solution Attempt 1: Frame Classification &amp; Time-Series Segmentation</a> <ul> <li><a href="#time-series-segmentation">Time Series Segmentation</a></li> <li><a href="#results-1">Results</a></li> </ul> </li> <li> <a href="#solution-attempt-2-working-prototype">Solution Attempt 2: Object Detection &amp; Post-Processing</a> <ul> <li><a href="#detecting-hand-object-contact">Detecting Hand-Object Contact</a></li> <li><a href="#filtering-experimenters-hand-using-color-detection">Filtering Experimenter’s Hand Using Color Detection</a></li> <li><a href="#comparison-with-image-segmentation-models">Comparison with Image Segmentation Models</a></li> <li><a href="#solution-attempt-2-results">Results</a></li> </ul> </li> <li> <a href="#next-steps">Next Steps</a> <ul> <li><a href="#alleviating-trial-initiation-prediction-errors">Alleviating Trial Initiation Prediction Errors</a></li> <li><a href="#alleviating-over-segmentation-errors">Alleviating Over-Segmentation Errors</a></li> <li><a href="#other-future-directions">Other Future Directions</a></li> </ul> </li> </ul> <h1 id="what-was-the-lab-studying">What was the lab studying?</h1> <p>My project investigated an area of computational neuroscience called the “Bayesian brain hypothesis.” The Bayesian brain hypothesis posits that the human brain is a machine which makes “Bayes-optimal” inferences: the statistically optimal combination of all present information and previous knowledge.</p> <p>My research group focused on Bayesian modeling of human tactile perception. We set up an experiment to examine whether the human brain unknowingly uses Bayesian inference to form an internal representation of probability distributions using tactile information. We had participants complete a haptic categorization task which entailed both learning and sustained performance after learning, and compared human performance to a series of Bayesian models which were simulated to complete the same task.</p> <p>This research on human subconscious statistical learning was very interesting, but the focus of this blogpost is on my journey towards using computer vision to automate 250+ hours of video analysis collected for this experiment. Here, I will give a brief overview of our experiment which will be sufficient to highlight the relevance and scope of computer vision in my research.</p> <div class="text-center mt-4"> <img src="/assets/img/procedure.png" alt="Procedure" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 1: Participant learning procedure</figcaption> </div> <p><br></p> <p>During the experiment, participants sat behind an opaque screen that hid their hands from view, allowing them to feel but not see the stimuli. On each trial, participants were presented with a randomly selected object from a collection of 25 novel stimuli. These objects are roughly the shape of a large coin, and differ by the number of sides and by the density of dots on one face. Participants had the goal on each trial to correctly classify the object into one of the novel categories “Elyk” or “Noek” (Figure 2). Participants were informed whether their classification was correct after each trial. At the start of the experiment, participants would guess randomly the category of each stimulus; however, as they repeatedly guessed a stimulus’s category and received feedback, they improved their performance over the course of the experiment.</p> <div class="text-center mt-4"> <img src="/assets/img/stimulus.png" alt="Stimulus Image" class="img-fluid rounded" style="max-width: 60%; height: auto;"> <figcaption>Figure 2: Rendering of sample stimulus which participants learned to classify</figcaption> </div> <p><br></p> <p>The learning task was inherently challenging because the “Elyk” and “Noek” categories we made for the objects were defined according to 2D Gaussian distributions (one dimension for number of sides, the other for dot spacing). Participants had 5 seconds to feel the stimulus on each trial, and were informed by a beep that they should put the object down when their time was up. Sometimes participants identified the category of the stimulus in less than 5 seconds, and sometimes they took longer than 5. This led us to wonder what factors influence how long participants feel the object on each trial. <b>In order to identify these factors, we needed to determine how long participants held stimuli in each trial.</b></p> <p>Manually labeling all of the videos we collected during the experiments is practically infeasible, so I aimed to create a computer vision system which could automatically detect the duration of each trial.</p> <h1 id="my-goal">My goal</h1> <p><b> How can I automatically segment video of a participant completing the experiment into distinct temporal regions: during a trial, where the participant is touching the object, and between trials, where the experimenter switches the object in front of the participant? </b></p> <div class="text-center mt-4"> <img src="/assets/img/TAS-goal.png" alt="Procedure" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 3: Goal of computer vision automation: given video of a participant repeatedly picking up and putting down stimuli, how can we temporally segment the video to determine how long the participant holds the stimulus on each trial?</figcaption> </div> <p><br></p> <h1 id="preliminary-work-automatic-video-zooming">Preliminary work: automatic video zooming</h1> <p>I made a simple tool to automatically zoom videos to only focus on hands. By ensuring that the hands were in the center of every video, I could remove most of the pixels irrelevant to the task, thus speeding up processing time. I used the mediapipe library to detect the hands in the video.</p> <div class="d-flex justify-content-center mt-3"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" controls=""> <source src="/assets/video/auto-crop-tool.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <h1 id="solution-attempt-1-unsuccessful">Solution attempt 1 (unsuccessful):</h1> <p>Not long before I realized that the lab would benefit from this computer vision system, I developed my <a href="https://what-bird-is-that.streamlit.app/" rel="external nofollow noopener" target="_blank">“What Bird is That” project</a>, which involved using a machine learning model which can identify the species of bird in a photo. The computer vision system for this project was a straightforward image classification model, one of the most foundational tasks in computer vision. In order to train that model, I used a dataset with thousands of photos of birds and labels indicating their species.</p> <p>With this experience in mind, I wondered if I could extend this approach, combined with time series analysis, as a technique to solve our problem. Of course, my bird species identifier only analyzed images, whereas this task for the lab involves video. Here is an overview of the method I proposed.</p> <ol> <li>Manually label frames in a training set as containing “hand-object contact”</li> <li>Train an image classification model to identify whether there is participant hand-object contact in an image</li> <li>Extract all of the frames from a video, and have the model assign probability of participant hand-object contact in each frame</li> <li>Use a time series segmentation algorithm on the time series of single-frame contact probabilities to determine sustained periods of contact and non-contact</li> </ol> <p>In pseudocode:</p> <div class="text-center mt-4"> <img src="/assets/img/initial-algo.png" alt="Algorithm 3" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 4: Algorithm describing initial attempt</figcaption> </div> <p><br></p> <h3 id="time-series-segmentation">Time series segmentation</h3> <p><b>Setup of the problem:</b> The output of the image classifier is a univariate time series representing the probability of contact in \(T\) frames, \(y=\{y_1, y_2,…,y_T \}\). Videos of participants were collected in groups of 5 trials, so there should be 10 changepoints in each video; however, the first time the participant picked up the stimulus was cut off at the start of some videos, resulting in 9 changepoints in those videos. I therefore conceptualized the problem as having \(K^*\) unknown changepoints \(\{\tau_1,\tau_2, …, \tau_{K^*} \}\) where \(K^* = 9\) or \(K^* = 10\).</p> <p>We hope that the time series of image probabilities generated by the image classifier alternates between two stationary distributions: one which occurs during a trial (during hand-object contact) and the other between trials:</p> <div class="text-center mt-4"> <img src="/assets/img/alternating-dists.png" alt="Pelt" class="img-fluid rounded" style="max-width: 70%; height: auto;"> <figcaption>Time series alternates between stationary distribution during a trial, and stationary distribution between trials</figcaption> </div> <p><br></p> <p>Truong et al. (2020) highlight that the Pruned Exact Linear Time (PELT) algorithm developed by Killick (2012) finds the optimal solution to the segmentation problem with unknown number of changepoints. Truong et al. (2024) also provide an easy-to-use Python package, <a href="https://centre-borelli.github.io/ruptures-docs/" rel="external nofollow noopener" target="_blank">ruptures</a>, which impliments PELT and other time series segmentation algorithms.</p> <div class="text-center mt-4"> <img src="/assets/img/pelt.png" alt="PELT" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 5: PELT algorithm for time series segmentation (reproduced from Truong et al. (2020))</figcaption> </div> <p><br></p> <h3 id="results">Results</h3> <p>I hand labeled transition frames for 4 minutes of an 8-minute 60fps video. I trained a similar image classifier to the one I used in my <a href="https://what-bird-is-that.streamlit.app/" rel="external nofollow noopener" target="_blank">“What Bird is That” project</a> (that project used EfficientNet, and I experimented with EfficientNet and EfficientNetV2 here).</p> <div class="text-center mt-4"> <img src="/assets/img/PELT_result.png" alt="PELT result" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 6: An example of segmented changepoints using the PELT algorithm. The black dotted lines represent predicted changepoints, and the blue and red regions represent segments between trials and during trials, respectively. The detected changepoints are clearly very close to the true transitions. The blue line, which represents probability of hand-object contact over time, was generated using the image classifier</figcaption> </div> <p><br></p> <p>I used the image classifier to predict the probability of contact in the portion of the video which was not in the training set. I then used PELT to segment this time series of predictions. The preliminary results looked very strong in this case where (Figure 6) I trained the image classifier on a segment of the test video.</p> <p>After these preliminary results, I trained the image classifier on video from 3 additional participants. Unfortunately, the image classifier trained on additional data didn’t generalize well to new participants.</p> <div class="text-center mt-4"> <img src="/assets/img/poor-generalization.png" alt="PELT result" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 7: (A) Probability of contact in video taken from the same source as the training data. The input video is the same as in Figure 6. (B) Image classifier trained on additional data performing inference on video from a participant who is not in the training data. It is clear that the image classifier does not perform well on this data.</figcaption> </div> <p><br></p> <h1 id="solution-attempt-2-working-prototype">Solution attempt 2 (working prototype)</h1> <p>Shan et al., 2020 introduced a dataset with 100,000 annotated images of hands in contact with objects. Their annotations include bounding boxes round the hands and the objects with which they are in contact. They distinguish between stationary objects (e.g. tables and furniture) and portable objects which can be moved by hands. They train a modified version of the popular Faster-RCNN object detection network (Ren et al., 2015) on their dataset, which obtains strong results.</p> <p>I applied the object detection model developed by Shan et al. (2020) on frames extracted from participant videos. I added a post-processing step to reduce false positives caused by the algorithm detecting the experimenter’s hand in contact with the stimulus.</p> <p>I had not added this step, then the algorithm would falsely report the hand-object contact associated with a participant being mid-trial when the experimenter is swapping the stimulus between trials. I identified frames in which an experimenter was holding the object by recognizing that experimenters wore blue latex gloves. Thus, if more than half of the pixels in a bounding box are blue, then I’d identify that the hand belongs to the experimenter.</p> <div class="text-center mt-4"> <img src="/assets/img/algo-1.png" alt="Main algorithm" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 9: The most important algorithm: identifying participant-object contact in video sequences</figcaption> </div> <p><br></p> <p>To account for different lighting conditions, I determined that a pixel was blue, and therefore part of an experimenter’s blue latex gloves, if the pixel has HSV color code between (90, 50, 50) and (130, 255, 255).</p> <div class="text-center mt-4"> <img src="/assets/img/blue_gradient.png" alt="Blue range" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 10: The range of blue values which we associate with the experimenter’s hands in blue latex gloves.</figcaption> </div> <p><br></p> <p>I also briefly tried the image segmentation model from Zhang et al. (2022) (which is shown in the video at the top of this page), although it had worse performance than the image classification model from Shan et al. (2020). My application doesn’t require the fine-grained pixel-wise predictions of image segmentation, so I stuck with the more accurate image classifier.</p> <h3 id="solution-attempt-2-results">Solution attempt 2 results:</h3> <div class="text-center mt-4"> <img src="/assets/img/frame-wise-output.png" alt="Main algorithm" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 11: Frame-wise predictions</figcaption> </div> <p><br></p> <p>As is common in the Temporal Action Segmentation literature (Ding et al., 2024), I used both a quantitative and qualitative evaluation to determine the quality of my results. My method had strong qualitative results, but was prone to “over segmentation errors,” which resulted in poor results on temporally sensitive quantitative evaluation metrics. This is clear when we look at whole-sequence qualitative results (Figure 12).</p> <div class="text-center mt-4"> <img src="/assets/img/sv2_tas.png" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"> </div> <div class="text-center mt-4"> <img src="/assets/img/sr2_tas.png" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 12: Sample results showing qualitative whole-sequence performance from 2 different participants.</figcaption> </div> <p><br></p> <p>From Figure 12, we can clearly see that on aggregate, the algorithm’s predictions are close to the ground truth. In fact, the algorithm correctly detected the class of 87% of the 87,352 labeled frames I tested. The small jitters in which the predicted class rapidly switch between mid-trial and between-trials are called “over segmentation errors,” and are a well-documented challenge in the temporal action section literature (Ishikawa et al., 2021; Xu et al., 2022; Ding et al., 2024). These over segmentation errors are the primary reason why I say that my algorithm exhibits strong <i>preliminary</i> results.</p> <p>The top result in Figure 12 demonstrates another type of common error: the model predicts that a trial begins later than it should. This error is partly a matter of definition of when a trial starts. After some discussion, my supervisor decided that we should denote the beginning of a trial as soon as the participant begins to touch the stimulus, rather than once they’ve lifted it off of the table.</p> <p>The model tends to erroneously predict that the trial starts late because at the start of hand-object contact, the model detects the participants’ hands resting on the table, but does not properly detect the stimulus which is on the table; rather, it predicts that the stimulus is part of the table while the participant has not lifted the stimulus off of it. The object detection model distinguishes between portable objects (like our stimuli) and stationary objects (like the table on which the participants rest their hands) (Shan et al., 2020). When the stimulus is still on the table and the participant is touching it, the object detection model can often draw a bounding box around the table, and predict that the participant is touching a stationary object: it fails to identify that the participant is touching a thin portable object which is itself in contact with a stationary object.</p> <h1 id="next-steps">Next steps</h1> <h3 id="alleviating-trial-initiation-prediction-errors">Alleviating trial initiation prediction errors</h3> <p>To alleviate this error, I think that I could train an object-detection algorithm specifically designed to recognize our stimuli. In this way, we could detect when there is overlap between a detected hand and a detected stimulus, facilitating the inference that there is hand-object contact in that frame. Creating training data for an object detection model is usually time-consuming; however, the current model from Shan et al. (2020) can already accurately identify our stimuli in many situations. The stimuli which are detected by the current object detection algorithm can be used as training data for a stimulus-specific object detector. I can use heuristics to improve the quality of these automatically generated labels to ensure that only quality samples are used to train this domain-specific object detector:</p> <ol> <li>Only accept bounding boxes for stimuli which have similar length and width. This object detector is intended to work.</li> <li>Only include objects detected which have a bounding smaller than hands detected in the smaller frame.</li> </ol> <p>These two heuristics would prevent the poorly-identified objects in Figure 13 from being included as training data for this new object detector.</p> <div class="text-center mt-4"> <img src="/assets/img/incorrectly-identified-objs.png" alt="incorrectly-identified-objects" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 13: Incorrectly identified objects which wouldn't erroneously be added to training data using the proposed filtering strategy.</figcaption> </div> <p><br></p> <h3 id="alleviating-over-segmentation-errors">Alleviating over segmentation errors</h3> <p>Over segmentation errors are the primary issue with the algorithm I developed. In my thesis, I proposed multiple methods which I expect would resolve the issue in this context.</p> <p>Some over-segmentation errors arise because our algorithm is not temporally-aware: it classifies each frame individually, without taking account of context from surrounding frames. An ideal system would be able to recognize that if a participant picks up the object, and manipulates it in a way which causes it to be briefly hidden from the camera, then they probably did not drop the object and pick it back up very quickly.</p> <p>In the temporal action segmentation literature, “timestamp supervision” refers to a specific type of semi-supervised learning, where most training data is unlabeled, but in each video, there is (at least) one frame with a class label and the label’s associated timestamp (Li et al., 2021, Ding et al., 2024).</p> <p>I propose that a temporal action segmentation model which requires timestamp supervision could be trained on a dataset that is automatically generated using my current system. A dataset with timestamp supervision does not require labels on every frame in videos which serve as training data. This is a key insight, and allows me to extract a small number of labels from the system’s predictions to form this training dataset.</p> <p>To automatically generate this training set, my current model will predict the class of every frame in the entire dataset, and extract only the frames where we are most confident about the model’s predicted labels according to a simple heuristic. This would be done on video for which there are no ground truth labels.</p> <p>Figure 14 shows both the model’s predicted labels and ground truth labels from a part of a video. The black dotted lines represent frames about which we have the highest prediction confidence because they are in the middle of a sequence of a single predicted class.</p> <div class="text-center mt-4"> <img src="/assets/img/sv5_dotted.png" alt="confident-regions" class="img-fluid rounded" style="max-width: 100%; height: auto;"> <figcaption>Figure 14: Black dotted lines represent frames for which we are very confident that their label is accurate because they are surrounded by many frames with the same prediction. These regions can be selectivley extracted to use as training data for a new model trained with "timestamp supervision."</figcaption> </div> <p><br></p> <p>This simple heuristic of only extracting frames which are closest to the middle of a long sequence would likely be sufficient to identify frames which mostly belong to their predicted class. Over-segmentation errors from the current model are unlikely to meaningfully affect this method of generating training data. This is important because these over-segmentation errors which this technique sidesteps are a primary factor which influence our model’s poor performance in temporally-aware evaluation metrics.</p> <h3 id="other-future-directions">Other future directions</h3> <p>If you’ve read this far, I’m very impressed! I also thought about a few other future directions including the following. Feel free to ask me about these:</p> <ol> <li>Image classifier-based method using I3D for temporally-sensitive feature extraction (Carreira &amp; Zisserman, 2018).</li> <li>Simple processing on existing outputs: smoothing the time series (perhaps with a convolution?), then apply time series changepoint detection with PELT or similar.</li> <li>Retrain the Shan et al. (2020) dataset but use feature extraction from I3D. This would likely help because Shan et al. were focused on object detection in single images, rather than sequences.</li> <li> Snorkel (Ratner et al., 2017) could be used in conjunction with my proposed methods for automatically generating training data for use with specialized models. <ul> <li>I need to learn more about Snorkel and how it quantifies uncertainty from automatic labelling functions.</li> </ul> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Noah Ripstein. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/pseudocode-setup.js?72ed28a041e322f764b0d9c59cf428b5"></script> <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>