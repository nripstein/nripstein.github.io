<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://nripstein.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nripstein.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-05T15:59:22+00:00</updated><id>https://nripstein.github.io/feed.xml</id><title type="html">blank</title><subtitle>Noah Ripstein&apos;s personal website </subtitle><entry><title type="html">Automating Repetetive Work With Computer Vision</title><link href="https://nripstein.github.io/blog/ug-thesis/" rel="alternate" type="text/html" title="Automating Repetetive Work With Computer Vision"/><published>2024-12-20T16:40:16+00:00</published><updated>2024-12-20T16:40:16+00:00</updated><id>https://nripstein.github.io/blog/ug-thesis</id><content type="html" xml:base="https://nripstein.github.io/blog/ug-thesis/"><![CDATA[<div class="d-flex justify-content-center mt-3"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" controls=""> <source src="/assets/video/segmentation.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> <h1 id="1-minute-tldr">1 minute TLDR</h1> <p>In my undergraduate honors thesis in computational neuroscience, I identified an opportunity to apply computer vision to automate 250+ hours of manual video analysis, streamlining a labor-intensive process. My lab focused on the “Bayesian brain hypothesis” and how humans subconsciously use Bayesian inference to form internal representations of probability distrubutions during sensory perception and learning. Participants in our study learned to novel classify objects based on touch, and I aimed to automatically segment video data into “during-trial” and “between-trial” regions to analyze the duration of hand-object contact.</p> <h4>Key Achievements:</h4> <ol> <li> <b>First Attempt: Frame Classification and Time-Series Segmentation</b> <ul> <li>Used an image classification model to detect "hand-object contact" frame by frame.</li> <li>Combined these frame-wise predictions with the PELT algorithm for time-series segmentation. Initial results were promising but the image classifier faced challenges generalizing to unseen participants.</li> </ul> </li> <li> <b>Second Attempt: Object Detection and Post-Processing</b> <ul> <li>Leveraged an object detection model trained on a public dataset, avoiding the need for hand-labelling training data.</li> <li>Developed domain-specific post-processing steps reduce false positives.</li> <li>Achieved 87% frame-wise classification accuracy and very strong qualitative results.</li> <li>Encountered "over-segmentation errors," causing poor performance on temporally-sensitive evaluation metrics.</li> </ul> </li> <li> <b>Promising future directions:</b> <ul> <li>Proposed a method to use existing models, which are trained on public datasets, to generate high quality training for specialized task-specific models based on open source models.</li> <li>I strongly suspect that carefully extracting data in the way I suggest for use as training data in the semi-supervised model I suggest would succesfully convert the strong frame-wise performance to strong whole-sequence performance.</li> </ul> </li> </ol> <h4>Results:</h4> <p>The prototype exhibited strong qualitative results, showing clear potential to automate video segmentation. Over-segmentation errors and trial start prediction inaccuracies highlight areas for refinement. Future directions include integrating temporally-aware open source models through the pipeline I propose which leverages automated data labeling and semi-supervised learning.</p> <p>This project demonstrates the value of cross-disciplinary problem-solving, combining various areas of machine learning and statistics (human-object interaction, temporal action segmentation, time series segmationation), to answer scientifically-relevant questions.</p> <div class="text-center mt-4"> <img src="/assets/img/sv2_tas.png" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Ground truth vs predicted by algorithm. The red and green sections line up well in the top and bottom figures, highlighting the method's strong performance</figcaption> </div> <p><br/> <br/> <br/></p> <h1>Table of Contents</h1> <ul> <li><a href="#1-minute-tldr">1-Minute TLDR</a></li> <li><a href="#what-was-the-lab-studying">What was the lab studying?</a> <ul> <li><a href="#experiment-overview">Experiment Overview</a></li> <li><a href="#stimuli-and-task">Stimuli and Task</a></li> <li><a href="#motivation-for-automation">Motivation for Automation</a></li> </ul> </li> <li><a href="#my-goal">My Goal</a></li> <li><a href="#preliminary-work-automatic-video-zooming">Preliminary Work: Automatic Video Zooming</a></li> <li><a href="#solution-attempt-1-unsuccessful">Solution Attempt 1: Frame Classification &amp; Time-Series Segmentation</a> <ul> <li><a href="#time-series-segmentation">Time Series Segmentation</a></li> <li><a href="#results-1">Results</a></li> </ul> </li> <li><a href="#solution-attempt-2-working-prototype">Solution Attempt 2: Object Detection &amp; Post-Processing</a> <ul> <li><a href="#detecting-hand-object-contact">Detecting Hand-Object Contact</a></li> <li><a href="#filtering-experimenters-hand-using-color-detection">Filtering Experimenter’s Hand Using Color Detection</a></li> <li><a href="#comparison-with-image-segmentation-models">Comparison with Image Segmentation Models</a></li> <li><a href="#solution-attempt-2-results">Results</a></li> </ul> </li> <li><a href="#next-steps">Next Steps</a> <ul> <li><a href="#alleviating-trial-initiation-prediction-errors">Alleviating Trial Initiation Prediction Errors</a></li> <li><a href="#alleviating-over-segmentation-errors">Alleviating Over-Segmentation Errors</a></li> <li><a href="#other-future-directions">Other Future Directions</a></li> </ul> </li> </ul> <h1 id="what-was-the-lab-studying">What was the lab studying?</h1> <p>My project investigated an area of computational neuroscience called the “Bayesian brain hypothesis.” The Bayesian brain hypothesis posits that the human brain is a machine which makes “Bayes-optimal” inferences: the statistically optimal combination of all present information and previous knowledge.</p> <p>My research group focused on Bayesian modeling of human tactile perception. We set up an experiment to examine whether the human brain unknowingly uses Bayesian inference to form an internal representation of probability distributions using tactile information. We had participants complete a haptic categorization task which entailed both learning and sustained performance after learning, and compared human performance to a series of Bayesian models which were simulated to complete the same task.</p> <p>This research on human subconscious statistical learning was very interesting, but the focus of this blogpost is on my journey towards using computer vision to automate 250+ hours of video analysis collected for this experiment. Here, I will give a brief overview of our experiment which will be sufficient to highlight the relevance and scope of computer vision in my research.</p> <div class="text-center mt-4"> <img src="/assets/img/procedure.png" alt="Procedure" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 1: Participant learning procedure</figcaption> </div> <p><br/></p> <p>During the experiment, participants sat behind an opaque screen that hid their hands from view, allowing them to feel but not see the stimuli. On each trial, participants were presented with a randomly selected object from a collection of 25 novel stimuli. These objects are roughly the shape of a large coin, and differ by the number of sides and by the density of dots on one face. Participants had the goal on each trial to correctly classify the object into one of the novel categories “Elyk” or “Noek” (Figure 2). Participants were informed whether their classification was correct after each trial. At the start of the experiment, participants would guess randomly the category of each stimulus; however, as they repeatedly guessed a stimulus’s category and received feedback, they improved their performance over the course of the experiment.</p> <div class="text-center mt-4"> <img src="/assets/img/stimulus.png" alt="Stimulus Image" class="img-fluid rounded" style="max-width: 60%; height: auto;"/> <figcaption>Figure 2: Rendering of sample stimulus which participants learned to classify</figcaption> </div> <p><br/></p> <p>The learning task was inherently challenging because the “Elyk” and “Noek” categories we made for the objects were defined according to 2D Gaussian distributions (one dimension for number of sides, the other for dot spacing). Participants had 5 seconds to feel the stimulus on each trial, and were informed by a beep that they should put the object down when their time was up. Sometimes participants identified the category of the stimulus in less than 5 seconds, and sometimes they took longer than 5. This led us to wonder what factors influence how long participants feel the object on each trial. <b>In order to identify these factors, we needed to determine how long participants held stimuli in each trial.</b></p> <p>Manually labeling all of the videos we collected during the experiments is practically infeasible, so I aimed to create a computer vision system which could automatically detect the duration of each trial.</p> <h1 id="my-goal">My goal</h1> <p><b> How can I automatically segment video of a participant completing the experiment into distinct temporal regions: during a trial, where the participant is touching the object, and between trials, where the experimenter switches the object in front of the participant? </b></p> <div class="text-center mt-4"> <img src="/assets/img/TAS-goal.png" alt="Procedure" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 3: Goal of computer vision automation: given video of a participant repeatedly picking up and putting down stimuli, how can we temporally segment the video to determine how long the participant holds the stimulus on each trial?</figcaption> </div> <p><br/></p> <h1 id="preliminary-work-automatic-video-zooming">Preliminary work: automatic video zooming</h1> <p>I made a simple tool to automatically zoom videos to only focus on hands. By ensuring that the hands were in the center of every video, I could remove most of the pixels irrelevant to the task, thus speeding up processing time. I used the mediapipe library to detect the hands in the video.</p> <div class="d-flex justify-content-center mt-3"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" controls=""> <source src="/assets/video/auto-crop-tool.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> <h1 id="solution-attempt-1-unsuccessful">Solution attempt 1 (unsuccessful):</h1> <p>Not long before I realized that the lab would benefit from this computer vision system, I developed my <a href="https://what-bird-is-that.streamlit.app/">“What Bird is That” project</a>, which involved using a machine learning model which can identify the species of bird in a photo. The computer vision system for this project was a straightforward image classification model, one of the most foundational tasks in computer vision. In order to train that model, I used a dataset with thousands of photos of birds and labels indicating their species.</p> <p>With this experience in mind, I wondered if I could extend this approach, combined with time series analysis, as a technique to solve our problem. Of course, my bird species identifier only analyzed images, whereas this task for the lab involves video. Here is an overview of the method I proposed.</p> <ol> <li>Manually label frames in a training set as containing “hand-object contact”</li> <li>Train an image classification model to identify whether there is participant hand-object contact in an image</li> <li>Extract all of the frames from a video, and have the model assign probability of participant hand-object contact in each frame</li> <li>Use a time series segmentation algorithm on the time series of single-frame contact probabilities to determine sustained periods of contact and non-contact</li> </ol> <p>In pseudocode:</p> <div class="text-center mt-4"> <img src="/assets/img/initial-algo.png" alt="Algorithm 3" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 4: Algorithm describing initial attempt</figcaption> </div> <p><br/></p> <h3 id="time-series-segmentation">Time series segmentation</h3> <p><b>Setup of the problem:</b> The output of the image classifier is a univariate time series representing the probability of contact in \(T\) frames, \(y=\{y_1, y_2,…,y_T \}\). Videos of participants were collected in groups of 5 trials, so there should be 10 changepoints in each video; however, the first time the participant picked up the stimulus was cut off at the start of some videos, resulting in 9 changepoints in those videos. I therefore conceptualized the problem as having \(K^*\) unknown changepoints \(\{\tau_1,\tau_2, …, \tau_{K^*} \}\) where \(K^* = 9\) or \(K^* = 10\).</p> <p>We hope that the time series of image probabilities generated by the image classifier alternates between two stationary distributions: one which occurs during a trial (during hand-object contact) and the other between trials:</p> <div class="text-center mt-4"> <img src="/assets/img/alternating-dists.png" alt="Pelt" class="img-fluid rounded" style="max-width: 70%; height: auto;"/> <figcaption>Time series alternates between stationary distribution during a trial, and stationary distribution between trials</figcaption> </div> <p><br/></p> <p>Truong et al. (2020) highlight that the Pruned Exact Linear Time (PELT) algorithm developed by Killick (2012) finds the optimal solution to the segmentation problem with unknown number of changepoints. Truong et al. (2024) also provide an easy-to-use Python package, <a href="https://centre-borelli.github.io/ruptures-docs/">ruptures</a>, which impliments PELT and other time series segmentation algorithms.</p> <div class="text-center mt-4"> <img src="/assets/img/pelt.png" alt="PELT" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 5: PELT algorithm for time series segmentation (reproduced from Truong et al. (2020))</figcaption> </div> <p><br/></p> <h3 id="results">Results</h3> <p>I hand labeled transition frames for 4 minutes of an 8-minute 60fps video. I trained a similar image classifier to the one I used in my <a href="https://what-bird-is-that.streamlit.app/">“What Bird is That” project</a> (that project used EfficientNet, and I experimented with EfficientNet and EfficientNetV2 here).</p> <div class="text-center mt-4"> <img src="/assets/img/pelt_result.png" alt="PELT result" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 6: An example of segmented changepoints using the PELT algorithm. The black dotted lines represent predicted changepoints, and the blue and red regions represent segments between trials and during trials, respectively. The detected changepoints are clearly very close to the true transitions. The blue line, which represents probability of hand-object contact over time, was generated using the image classifier</figcaption> </div> <p><br/></p> <p>I used the image classifier to predict the probability of contact in the portion of the video which was not in the training set. I then used PELT to segment this time series of predictions. The preliminary results looked very strong in this case where (Figure 6) I trained the image classifier on a segment of the test video.</p> <p>After these preliminary results, I trained the image classifier on video from 3 additional participants. Unfortunately, the image classifier trained on additional data didn’t generalize well to new participants.</p> <div class="text-center mt-4"> <img src="/assets/img/poor-generalization.png" alt="PELT result" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 7: (A) Probability of contact in video taken from the same source as the training data. The input video is the same as in Figure 6. (B) Image classifier trained on additional data performing inference on video from a participant who is not in the training data. It is clear that the image classifier does not perform well on this data.</figcaption> </div> <p><br/></p> <h1 id="solution-attempt-2-working-prototype">Solution attempt 2 (working prototype)</h1> <p>Shan et al., 2020 introduced a dataset with 100,000 annotated images of hands in contact with objects. Their annotations include bounding boxes round the hands and the objects with which they are in contact. They distinguish between stationary objects (e.g. tables and furniture) and portable objects which can be moved by hands. They train a modified version of the popular Faster-RCNN object detection network (Ren et al., 2015) on their dataset, which obtains strong results.</p> <p>I applied the object detection model developed by Shan et al. (2020) on frames extracted from participant videos. I added a post-processing step to reduce false positives caused by the algorithm detecting the experimenter’s hand in contact with the stimulus.</p> <p>I had not added this step, then the algorithm would falsely report the hand-object contact associated with a participant being mid-trial when the experimenter is swapping the stimulus between trials. I identified frames in which an experimenter was holding the object by recognizing that experimenters wore blue latex gloves. Thus, if more than half of the pixels in a bounding box are blue, then I’d identify that the hand belongs to the experimenter.</p> <div class="text-center mt-4"> <img src="/assets/img/algo-1.png" alt="Main algorithm" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 9: The most important algorithm: identifying participant-object contact in video sequences</figcaption> </div> <p><br/></p> <p>To account for different lighting conditions, I determined that a pixel was blue, and therefore part of an experimenter’s blue latex gloves, if the pixel has HSV color code between (90, 50, 50) and (130, 255, 255).</p> <div class="text-center mt-4"> <img src="/assets/img/blue_gradient.png" alt="Blue range" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 10: The range of blue values which we associate with the experimenter’s hands in blue latex gloves.</figcaption> </div> <p><br/></p> <p>I also briefly tried the image segmentation model from Zhang et al. (2022) (which is shown in the video at the top of this page), although it had worse performance than the image classification model from Shan et al. (2020). My application doesn’t require the fine-grained pixel-wise predictions of image segmentation, so I stuck with the more accurate image classifier.</p> <h3 id="solution-attempt-2-results">Solution attempt 2 results:</h3> <div class="text-center mt-4"> <img src="/assets/img/frame-wise-output.png" alt="Main algorithm" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 11: Frame-wise predictions</figcaption> </div> <p><br/></p> <p>As is common in the Temporal Action Segmentation literature (Ding et al., 2024), I used both a quantitative and qualitative evaluation to determine the quality of my results. My method had strong qualitative results, but was prone to “over segmentation errors,” which resulted in poor results on temporally sensitive quantitative evaluation metrics. This is clear when we look at whole-sequence qualitative results (Figure 12).</p> <div class="text-center mt-4"> <img src="/assets/img/sv2_tas.png" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> </div> <div class="text-center mt-4"> <img src="/assets/img/sr2_tas.png" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 12: Sample results showing qualitative whole-sequence performance from 2 different participants.</figcaption> </div> <p><br/></p> <p>From Figure 12, we can clearly see that on aggregate, the algorithm’s predictions are close to the ground truth. In fact, the algorithm correctly detected the class of 87% of the 87,352 labeled frames I tested. The small jitters in which the predicted class rapidly switch between mid-trial and between-trials are called “over segmentation errors,” and are a well-documented challenge in the temporal action section literature (Ishikawa et al., 2021; Xu et al., 2022; Ding et al., 2024). These over segmentation errors are the primary reason why I say that my algorithm exhibits strong <i>preliminary</i> results.</p> <p>The top result in Figure 12 demonstrates another type of common error: the model predicts that a trial begins later than it should. This error is partly a matter of definition of when a trial starts. After some discussion, my supervisor decided that we should denote the beginning of a trial as soon as the participant begins to touch the stimulus, rather than once they’ve lifted it off of the table.</p> <p>The model tends to erroneously predict that the trial starts late because at the start of hand-object contact, the model detects the participants’ hands resting on the table, but does not properly detect the stimulus which is on the table; rather, it predicts that the stimulus is part of the table while the participant has not lifted the stimulus off of it. The object detection model distinguishes between portable objects (like our stimuli) and stationary objects (like the table on which the participants rest their hands) (Shan et al., 2020). When the stimulus is still on the table and the participant is touching it, the object detection model can often draw a bounding box around the table, and predict that the participant is touching a stationary object: it fails to identify that the participant is touching a thin portable object which is itself in contact with a stationary object.</p> <h1 id="next-steps">Next steps</h1> <h3 id="alleviating-trial-initiation-prediction-errors">Alleviating trial initiation prediction errors</h3> <p>To alleviate this error, I think that I could train an object-detection algorithm specifically designed to recognize our stimuli. In this way, we could detect when there is overlap between a detected hand and a detected stimulus, facilitating the inference that there is hand-object contact in that frame. Creating training data for an object detection model is usually time-consuming; however, the current model from Shan et al. (2020) can already accurately identify our stimuli in many situations. The stimuli which are detected by the current object detection algorithm can be used as training data for a stimulus-specific object detector. I can use heuristics to improve the quality of these automatically generated labels to ensure that only quality samples are used to train this domain-specific object detector:</p> <ol> <li>Only accept bounding boxes for stimuli which have similar length and width. This object detector is intended to work.</li> <li>Only include objects detected which have a bounding smaller than hands detected in the smaller frame.</li> </ol> <p>These two heuristics would prevent the poorly-identified objects in Figure 13 from being included as training data for this new object detector.</p> <div class="text-center mt-4"> <img src="/assets/img/incorrectly-identified-objs.png" alt="incorrectly-identified-objects" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 13: Incorrectly identified objects which wouldn't erroneously be added to training data using the proposed filtering strategy.</figcaption> </div> <p><br/></p> <h3 id="alleviating-over-segmentation-errors">Alleviating over segmentation errors</h3> <p>Over segmentation errors are the primary issue with the algorithm I developed. In my thesis, I proposed multiple methods which I expect would resolve the issue in this context.</p> <p>Some over-segmentation errors arise because our algorithm is not temporally-aware: it classifies each frame individually, without taking account of context from surrounding frames. An ideal system would be able to recognize that if a participant picks up the object, and manipulates it in a way which causes it to be briefly hidden from the camera, then they probably did not drop the object and pick it back up very quickly.</p> <p>In the temporal action segmentation literature, “timestamp supervision” refers to a specific type of semi-supervised learning, where most training data is unlabeled, but in each video, there is (at least) one frame with a class label and the label’s associated timestamp (Li et al., 2021, Ding et al., 2024).</p> <p>I propose that a temporal action segmentation model which requires timestamp supervision could be trained on a dataset that is automatically generated using my current system. A dataset with timestamp supervision does not require labels on every frame in videos which serve as training data. This is a key insight, and allows me to extract a small number of labels from the system’s predictions to form this training dataset.</p> <p>To automatically generate this training set, my current model will predict the class of every frame in the entire dataset, and extract only the frames where we are most confident about the model’s predicted labels according to a simple heuristic. This would be done on video for which there are no ground truth labels.</p> <p>Figure 14 shows both the model’s predicted labels and ground truth labels from a part of a video. The black dotted lines represent frames about which we have the highest prediction confidence because they are in the middle of a sequence of a single predicted class.</p> <div class="text-center mt-4"> <img src="/assets/img/sv5_dotted.png" alt="confident-regions" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 14: Black dotted lines represent frames for which we are very confident that their label is accurate because they are surrounded by many frames with the same prediction. These regions can be selectivley extracted to use as training data for a new model trained with "timestamp supervision."</figcaption> </div> <p><br/></p> <p>This simple heuristic of only extracting frames which are closest to the middle of a long sequence would likely be sufficient to identify frames which mostly belong to their predicted class. Over-segmentation errors from the current model are unlikely to meaningfully affect this method of generating training data. This is important because these over-segmentation errors which this technique sidesteps are a primary factor which influence our model’s poor performance in temporally-aware evaluation metrics.</p> <h3 id="other-future-directions">Other future directions</h3> <p>If you’ve read this far, I’m very impressed! I also thought about a few other future directions including the following. Feel free to ask me about these:</p> <ol> <li>Image classifier-based method using I3D for temporally-sensitive feature extraction (Carreira &amp; Zisserman, 2018).</li> <li>Simple processing on existing outputs: smoothing the time series (perhaps with a convolution?), then apply time series changepoint detection with PELT or similar.</li> <li>Retrain the Shan et al. (2020) dataset but use feature extraction from I3D. This would likely help because Shan et al. were focused on object detection in single images, rather than sequences.</li> <li> Snorkel (Ratner et al., 2017) could be used in conjunction with my proposed methods for automatically generating training data for use with specialized models. <ul> <li>I need to learn more about Snorkel and how it quantifies uncertainty from automatic labelling functions.</li> </ul> </li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[My journey towards automating 250 hours of manual video labelling for my undergrad thesis lab and boosting my team's productivity. (And don't worry, there's a 1 minute TLDR overview)]]></summary></entry><entry><title type="html">2048 Game AI</title><link href="https://nripstein.github.io/blog/2048-AI/" rel="alternate" type="text/html" title="2048 Game AI"/><published>2023-07-01T16:40:16+00:00</published><updated>2023-07-01T16:40:16+00:00</updated><id>https://nripstein.github.io/blog/2048-AI</id><content type="html" xml:base="https://nripstein.github.io/blog/2048-AI/"><![CDATA[<p>2048 is an online/mobile game which was originally released in 2014. The game begins with two randomly placed tiles, each having a value of either 2 or 4, randomly placed on a 4x4 grid. The player can move the tiles in four directions: up, down, left, or right. When a direction is chosen, all tiles on the grid slide as far as possible in that direction, merging with any adjacent tiles of the same value to form a new tile with double the value. The value of the new tile is added to the score. After the player’s turn, a new tile spawns in a random location; this new tile has a 90% chance of being a 2, and a 10% chance of being a 4. The game ends when the board is filled with tiles and the player has no legal moves. The goal of the game is to combine tiles until the 2048 tile is reached (although it is possible to continue playing after winning).</p> <p>I first implemented A Pure Monte Carlo Tree Search algorithm, and then improved performance by formalizing the game’s state tree as a Markov Decision Process. The final algorithm I designed achieves the 2048 tile 97% of the time and the 4096 tile 58% of the time.</p> <style>.figure-container{display:flex;justify-content:space-between;align-items:center;margin:20px 0}.subfigure{width:48%;text-align:center}.subfigure img{width:100%;height:auto}</style> <div class="figure-container"> <div class="subfigure"> <img src="/assets/img/2048-og.jpeg" alt="Original Game"/> <p>Original Game</p> </div> <div class="subfigure"> <img src="/assets/img/2048-mine.png" alt="Python Clone"/> <p>My Python Clone</p> </div> </div> <center> <p>Screenshots from original game and my implementation</p> </center> <p><br/></p> <h2 id="the-decision-tree">The decision tree</h2> <div class="text-center mt-4"> <img src="/assets/img/tree_1.jpeg" alt="decision tree" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> <figcaption>Figure 1: Decision tree representing possible board states after each move</figcaption> </div> <p><br/></p> <p>One of the challenges of creating an AI which plays 2048 is the sheer number of possible games. Figure 1 represents the possible board positions after the player makes only one move. If there are 8 free spaces on the board, for example, then there are 64 possible game states after the player’s move (assuming each of left, right, up and down are legal moves which do not combine any tiles). In general, there are 2(|<i>S</i>|)(<i>m</i>) + <i>c</i> possible states after the player’s move, where |<i>S</i>| is the number of legal moves, <i>m</i> is the number of empty spaces on the board after tiles have been combined from the player’s turn, and <i>c</i> is the number of tiles which get merged as a result of the player’s move.</p> <h2 id="ai-designs">AI designs</h2> <p>The initial algorithm I developed is a simple Pure Monte Carlo Tree Search (Algorithm 1). This algorithm takes the current game position and the desired number of simulations per direction ($n$) as inputs. It explores all legal next moves from the current position by simulating $n$ games for each potential move. Each of these simulated games is played by making random moves until the game is over. The scores from the end of these simulated games are then averaged to determine the quality of each move. The direction with the highest average score is selected:</p> <p>\begin{equation} \text{Selected move} = argmax_{move}(PMTCS(move)) \end{equation}</p> <p>This approach initiates from the green nodes in the game tree diagram (Fig. 1). From there, the algorithm proceeds through random exploration to reach the red child nodes, representing the spawning of a 2 or 4 tile in each possible location.</p> <p>While this approach provides a comprehensive exploration of the game tree, it has significant limitations. The primary concern lies in the random nature of the search process. As a consequence, some of the simulated games performed during the Monte Carlo simulations may yield exceptionally poor results that are highly unlikely to occur in actual gameplay. This lead me to want to discard a portion of those simulated games with particularly poor scores from consideration.</p> <p>Simply modifying Algorithm 1 to calculate the average score for a given move using only top-performing of simulated games would not adequately address this source of randomness, however. There are two sources of randomness inherent in the Pure Monte Carlo Tree Search: randomness associated with game-play (which I aim to reduce), and randomness of tile spawns. Discarding randomly played games with low scores in an attempt to address the former source of randomness might prevent the AI from evaluating branches of the tree which involve unlucky tiles spawning after the next turn.</p> <div class="text-center mt-4"> <img src="/assets/img/mcts.png" alt="Monte Carlo Tree search algorithm" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> </div> <p><br/></p> <h2 id="markov-decision-process">Markov decision process</h2> <p>Some of the pitfalls of the Pure Monte Carlo Tree Search can be circumvented by formalizing the game structure as a Markov Decision Process (MDP). The core functionality of the MCTS is used as part of the MDP-based strategy, but the evaluations made with MCTS are made more beneficial by explicitly maximizing expected value. Where the PMCTS algorithm begins Monte Carlo simulations from the board state after a player’s move (green nodes in Figure 1), The MDP strategy begins Monte Carlo simulations from each possible tile spawn in response to a player’s move (red nodes in Figure 1). It is possible to model the game as a Markov decision process because the game satisfies the Markov property: the probability of reaching a future state depends only on the current state, not on previous states:</p> \[P(s|s_{n-1}, s_{n-2},...,s_{0}) = P(s|s_{n-1})\] <p>In general, an is MDP is characterized as follows:</p> <ol> <li><em>S</em>: The set of states (board position and score) which could arise after the AI’s next move.</li> <li><em>A</em>: The set of actions which the AI could legally take from the current position.</li> <li><em>P(s</em>’|<em>s, a)</em>: Transition probability of reaching state <em>s’</em> given current state <em>s</em> ∈ <em>S</em> after taking action <em>a</em> ∈ <em>A</em>.</li> <li><em>V(s)</em>: The value function which determines the expected future reward associated with entering a state <em>s</em> ∈ <em>S</em>.</li> <li><em>π(s)</em>: The policy function which uses the other functions to strategically pick an action <em>a</em> ∈ <em>A</em> given any board state.</li> <li><em>R(s)</em>: The immediate reward associated with taking action <em>a</em> ∈ <em>A</em> which leads to state <em>s</em> ∈ <em>S</em> is a part of many MDP algorithms.</li> </ol> <p>\(A\) can be constructed by checking which of left, right, up or down are legal moves. $S$ can be constructed by placing a 2 and then a 4 on each empty tile for each \(a \in A\) (Algorithm 2). The value function, \(V(s, \vec{\theta})\), (Algorithm 3) performs a Monte Carlo tree search, with the number of simulations determined by the parameter vector $\vec{\theta}$. $\vec{\theta}$ contains the following parameters:</p> <ol> <li>Number of random searches for 2 spawning with 1-3 empty tiles.</li> <li>Number of random searches for 2 spawning with 4-6 empty tiles.</li> <li>Number of random searches for 2 spawning with 7-9 empty tiles.</li> <li>Number of random searches for 2 spawning with 10-15 empty tiles.</li> <li>How many times more searches 2 spawns should get compared to 4 spawns.</li> <li>Top proportion of best performing moves to include for score evaluation.</li> </ol> <p>$\theta_1, \theta_2, \theta_3$ are scaled so their respective empty tile ranges have the same total number of simulations. $\theta_4$ uses the same number of simulations for each of 10-15 empty tiles. If $\theta_1 = 500$, for example, the Monte Carlo simulation will run 500 times when there is one tile, 250 times when there are two, and 125 when there are three; if $\theta_4 = 20$, then the Monte Carlo simulation will run 20 times for any number of empty tiles $\in [10, 15s]$.</p> <p>$\vec{\theta}$ remains constant throughout a single play-through of the game. The policy function, which determines what action $a \in A$ to take, $\pi(s)$, is given by Equation 2:</p> \[\begin{equation} \pi(s) = \arg\max\limits_{a}\left( \sum\limits_{s' \in S} P(s'|s, a)V(s', \vec{\theta}) \right) \end{equation}\] <p>Notice that $\pi(s)$ picks the action with the highest expected value, where the value associated with reaching a state is given by a Monte Carlo tree search. Compared to the Pure Monte Carlo Tree Search, the MDP-based algorithm facilitates more sophisticated inference in two ways.</p> <ol> <li> <p>By discarding a portion of explored games with poor results at each node, the impact of the Monte Carlo search playing out extremely poor moves due to chance can be mitigated. Crucially, simulations with lower scores due to “unlucky” tile spawns in the subsequent turn are not eliminated, ensuring a more comprehensive exploration of potential game outcomes.</p> </li> <li> <p>Unlike the Pure Monte Carlo Tree Search, where game states in which a 2 spawns after the player’s turn are explored 9x as frequently as those in which a 4 spawns, the number of Monte Carlo searches on each node type can be made independent. This can guarantee that all nodes are explored at least 3 times, which gives information with far lower uncertainty than a node being explored 1 time, while not significantly increasing runtime.</p> </li> </ol> <p>The MDP-based algorithm is implemented in a manner which makes customizable the number of Monte Carlo searches per node ($\theta_1 - \theta_5$) and proportion of top-performing simulated scores to keep ($\theta_6$). Each of these parameters impact the reported score associated with a direction, and therefore the move selected by $\pi(s)$.</p> <div class="text-center mt-4"> <img src="/assets/img/2048-algo2.png" alt="algo 2" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> </div> <p><br/></p> <div class="text-center mt-4"> <img src="/assets/img/2048-algo3.png" alt="algo 3" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> </div> <p><br/></p> <h2 id="the-problem-with-bellmans-equation">The Problem with Bellman’s Equation</h2> <p>A common approach in MDP applications is to use Bellman’s Equation to evaluate the utility of taking action $a$. Rather than the utility of entering state $s^\prime$ being given by $V(s^\prime)$, Bellman’s equation would hold that the utility would be given by $R(s, a) + \gamma V(s^\prime)$. Here, $R(s, a)$ is the immediate guaranteed value associated with taking action $a$ when in state $s$, and $\gamma \in [0, 1]$ is a discount rate for expected future rewards. In principle, and in many other applications of MDP, this approach outperforms simply using $V(s^\prime)$, because it adheres to the principle that immediate, certain rewards are more favourable than uncertain future rewards. Equation \ref{Bellman_policy} shows how Bellman’s Equation could be incorporated into the policy function, $\pi (s)$.</p> <p>Despite the theoretical backing, using Equation \ref{Bellman_policy} as the policy function decreased performance compared to Equation \ref{MC12_policy_fn} (tested with $\gamma$ = 0.9) and brought performance roughly in line with the less computationally intense Pure Monte Carlo Tree Search. This is likely because the random games explored using the Monte Carlo value function tend to end with a score only marginally higher than the current score. This leads immediate score gain, $R(s, a)$, to have a much larger impact on the policy than future scores in Equation \ref{Bellman_policy}. This is problematic because it makes the algorithm more “greedy,” and prioritizes immediate rewards too much over future rewards (even with high values of $\gamma$).</p> <p>\begin{equation} \label{Bellman_policy} \pi(s) = \arg\max\limits_{a}\left( \sum\limits_{s’ \in S} P(s’|s, a)(R(s,a) + \gamma V(s’, \vec{\theta})) \right) \end{equation}</p> <h2 id="results-not-filled-in">Results (NOT FILLED IN)</h2> <div class="text-center mt-4"> <img src="/assets/img/2048-violin.png" alt="violin plot of results" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> </div> <p><br/></p> <h2 id="discussion">Discussion</h2> <p>Model score is most strongly influenced by the proportion of time the 4096 tile is reached. Many trials (with a wide range of parameters) which do not reach the 4096 tile achieve a score around 36,000, when they are near the 4096 tile. AI strategies are likely to fail shortly before reaching the next milestone tile because this is when there are the most high tiles on the board which can not yet be joined. Once reaching the 4096 tile, it is uncommon for a score below 60,000 to be reached, although there is a meaningful distribution between 60,000 and 80,000, where 80,000 represents the point near achieving a 8192 tile. This distribution of results requires that a given set of parameters is evaluated multiple times on the model before reporting performance. In order to illustrate this, consider the following example:</p> <p>Suppose that a set of parameters $\vec{\theta_1}$ has a true probability of reaching the 4096 tile 60\% of the time, with the model scoring 35,000 when it doesn’t reach 4096, and 70,000 when it does. This means that a score of $56,000 = 0.6(70,000) + 0.4(35,000)$ should be reported as the long term expected performance of $\vec{\theta_1}$. If $\vec{\theta_1}$ is tested 3 times, the probability of 0.375 that the AI would report a score of 47,333 by reaching 4096 only once in the three runs. With 9 repeats, the probability of 47,333 or a worse score being reported (reaching 4096 3 times or fewer of 9) is 0.099. Running the parameters 9 times drastically reduces the noisiness of the data, which provides a much more accurate representation of parameter performance. This enables the Bayesian optimization model (as discussed in section 6.1) to make more accurate predictions about how future sets of parameters will perform.</p> <h2 id="future-directions">Future directions</h2> <h3 id="parameter-optimization">Parameter optimization</h3> <p>As of the time of writing (summer 2023), this is an ongoing project, and the next thing I hope to implement is the optimization of $\vec{\theta}$, the parameter vector which controls how many random Monte Carlo searches to perform for a given number of empty tiles. The goal is to find the optimal tradeoff of time vs score. Below is a rough outline of the plan:</p> <ol> <li>Use a Quasi-Monte Carlo sampling technique with low discrepancy (such as Latin Hypercube Sampling or Sobol sampling) to draw a series of samples within the parameter space.</li> <li>Use Bayesian Optimization to determine which sets of parameters to evaluate next. (Implementation details such as Acquisition function not yet sorted out)</li> </ol> <p>I intend to optimize for both move speed and score, and find the pareto efficient solution set which lets me choose the optimal speed and average score of the final model. Bayesian optimization is likely a strong framework for parameter optimization because it calls the target function (which can be non-differentiable) fewer times than other methods like evolutionary algorithms.</p> <h3 id="dynamic-policy-switching">Dynamic policy switching</h3> <p>It is possible that when there are few tiles on the board, and explicit exploration of greater depth would be beneficial. Dynamically switching to an Expectimax algorithm seems like it may have strong results here. Expectimax uses Equation \ref{Bellman_policy} with a recursive value function given by Equation \ref{expectimax_value_fn}.</p> <p>\begin{equation} \label{expectimax_value_fn} V(s, \vec{\theta}) = \sum_{s^\prime \in S} P(s^\prime|s, a)(R(s, s^\prime) + \gamma V(s^\prime, \vec{\theta})) \end{equation}</p> <p>Equation \ref{expectimax_value_fn} would evaluate to a certain recursive depth, and then call Algorithm \ref{value_function} as the final evaulation of $V(s, \vec{\theta})$ to estimate the value at the final search state. This strategy would be particularly useful when there are few open tiles on the board because the low number of states to search increases the maximum search depth in a given amount of time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I developed a Python clone of the 2014 viral game "2048," and designed a probabilistic AI to play it. My algorithm achieves a 97% win-rate by formalizing the game as a Markov Decision Processes and using a hybrid of Monte Carlo tree search and expectimax to select moves.]]></summary></entry><entry><title type="html">What Bird is That? - Computer Vision Website</title><link href="https://nripstein.github.io/blog/bird/" rel="alternate" type="text/html" title="What Bird is That? - Computer Vision Website"/><published>2023-06-15T16:40:16+00:00</published><updated>2023-06-15T16:40:16+00:00</updated><id>https://nripstein.github.io/blog/bird</id><content type="html" xml:base="https://nripstein.github.io/blog/bird/"><![CDATA[<h2 id="project-overview">Project overview</h2> <p>This project aims to classify bird species using deep learning. Users can upload a photo of a bird, and the website will predict the species. The website then displays some information about the bird, which is pulled from the Wikipedia API. The model can identify over 500 species of bird with 97+% accuracy. The Bird Classifier leverages feature extraction and fine-tuning of the EfficientNetB4 model, pre-trained on the ImageNet dataset, to accurately identify and provide information about various bird species.</p> <h2 id="model-design">Model design</h2> <p><b>Data augmentation:</b> Data augmentation is a powerful technique employed to increase the diversity and variability of the training dataset. The augmentation techniques employed include random horizontal flipping, random height and width shifting, random zooming, random rotation, and random contrast adjustment. By randomly applying these operations to each image the model learns from during training, the model becomes more resilient to variations in bird pose, lighting conditions, and other factors. This augmentation process enhances the model’s ability to generalize well and accurately classify bird species under different circumstances.</p> <p><b>Feature extraction with EfficientNet:</b> Transfer learning is employed to leverage the knowledge gained from the extensive training on the large-scale ImageNet dataset. The EfficientNet architecture, known for its excellent performance and low training time in image classification tasks, serves as the backbone of the model. Feature extraction uses this backbone model architecture and weights, but adds a few layers which are trained on the bird image dataset.</p> <p><b>Fine-tuning the model:</b> After training the new layers during feature extraction, the weights of the last 10 layers of the EfficientNet model are unfrozen and trained on the bird images for an additional 5 epochs (with a reduced learning rate). This keeps most of learned features within those layers the same, but slightly adjusts them to perform better at classifying birds specifically.</p> <style>table{border-collapse:collapse;width:100%;font-size:40px;text-align:center}td,th{border:1px solid black;padding:12px}tr:hover{background-color:#f1f1f1}</style> <table> <tr> <td>Accuracy</td> <td>97.83%</td> </tr> <tr> <td>Precision</td> <td>98.19%</td> </tr> <tr> <td>Recall</td> <td>97.82%</td> </tr> <tr> <td>F1 Score</td> <td>97.79%</td> </tr> </table> <p>In addition to the quantitative evaluation from the test dataset, I also conducted a number of tests using photos from the internet, and photos I took myself, with excellent results. I was pleased to see that the website is even effective with blurry photos, like this one of a Wood Duck:</p> <div class="text-center mt-4"> <img src="/assets/img/wood-duck-detected.jpeg" alt="whole-sequence qualitative result 1" class="img-fluid rounded" style="max-width: 100%; height: auto;"/> </div> <p><br/></p> <h2 id="streamlit-website-deployment">Streamlit Website Deployment</h2> <div class="d-flex justify-content-center mt-3"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" controls=""> <source src="/assets/video/bird-demo-vid.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> <p>To provide a user-friendly interface for bird classification, I developed a Streamlit web application. Users can easily upload their bird photos and obtain predictions from the trained Bird Classifier model. The website presents a bar plot of the probability of the top 3 species (with their labels serving as Wikipedia links). After identifying the top prediction, the website presents a photo of the bird from the test dataset and provides details about the recognized species (from the Wikipedia API).</p> <h2 id="data">Data</h2> <p>The bird dataset used in this project comprises a wide range of bird species. It includes 525 different species, enabling the model to accurately identify and classify a diverse range of birds. Below are some sample photos from the dataset.</p> <style>.no-border-table{border:0;text-align:center}.no-border-table img{width:100%;height:auto;max-width:200px}.no-border-table table{border-collapse:collapse;width:100%;font-size:18px}.no-border-table th,.no-border-table td{border:0;padding:8px;text-align:center}</style> <div align="center" class="no-border-table"> <table> <tr> <td><img src="/assets/img/scarlet-macaw-square.jpg" alt="Scarlet Macaw"/></td> <td><img src="/assets/img/bald-eagle-square.webp" alt="Bald Eagle"/></td> <td><img src="/assets/img/blue-dacnis-square.jpg" alt="Blue Dacnis"/></td> </tr> <tr> <td>Scarlet Macaw</td> <td>Bald Eagle</td> <td>Blue Dacnis</td> </tr> </table> </div> <h2 id="code-and-live-website-links">Code and live website links</h2> <p><a href="https://what-bird-is-that.streamlit.app" style="font-size: 24px;">Website live link</a></p> <p>Github repo:</p> <p><a href="https://github.com/nripstein/what-bird-is-that"><img src="https://github-readme-stats.vercel.app/api/pin/?username=nripstein&amp;repo=what-bird-is-that&amp;theme=dark" alt="Readme Card"/></a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A simple website which identifies the species of bird in user-uploaded photos. After identifying the species, it provides the user with some information about the bird. Built to teach myself the basics of computer vision]]></summary></entry></feed>